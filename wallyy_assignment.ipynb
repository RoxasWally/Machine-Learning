{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjBtOHGKPK9O"
      },
      "source": [
        "# Final\n",
        "\n",
        "The final project will consist of a comparison between several CNN architectures for tumor detection. The goal is both to create a high-performing algorithm for differentiating kidneys with tumor from those that are normal, as well as to analyze performance across several different architecture permutations. In total, three different network designs will be tested. As each model is built and trained, ensure to serialize the final model `*.hdf5` file before moving to the next iteration.\n",
        "\n",
        "This assignment is part of the class **Introduction to Deep Learning for Medical Imaging** at University of California Irvine (CS190); more information can be found: https://github.com/peterchang77/dl_tutor/tree/master/cs190."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZSjWFxvPK9P"
      },
      "source": [
        "### Submission\n",
        "\n",
        "Once complete, the following items must be submitted:\n",
        "\n",
        "* final `*.ipynb` notebook\n",
        "* final trained `*.hdf5` model files for all three models\n",
        "* final compiled `*.csv` file with performance statistics across the different architectures\n",
        "* final 1-page write-up with methods and results of experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56d3oMiMw8Wm"
      },
      "source": [
        "# Google Colab\n",
        "\n",
        "The following lines of code will configure your Google Colab environment for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTFREDVBPK9S"
      },
      "source": [
        "### Enable GPU runtime\n",
        "\n",
        "Use the following instructions to switch the default Colab instance into a GPU-enabled runtime:\n",
        "\n",
        "```\n",
        "Runtime > Change runtime type > Hardware accelerator > GPU\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azn6DhZyPK9S"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwtBkGs3PK9T"
      },
      "source": [
        "### Jarvis library\n",
        "\n",
        "In this notebook we will Jarvis, a custom Python package to facilitate data science and deep learning for healthcare. Among other things, this library will be used for low-level data management, stratification and visualization of high-dimensional medical data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9jHHiCfvPK9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eb1895f-6f05-451b-d149-d4ef26f104de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jarvis-md\n",
            "  Downloading jarvis_md-0.0.1a17-py3-none-any.whl (89 kB)\n",
            "\u001b[K     |████████████████████████████████| 89 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.2\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 31.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (1.21.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (3.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from jarvis-md) (1.3.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->jarvis-md) (1.5.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->jarvis-md) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->jarvis-md) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->jarvis-md) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->jarvis-md) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->jarvis-md) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->jarvis-md) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->jarvis-md) (2022.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->jarvis-md) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->jarvis-md) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->jarvis-md) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->jarvis-md) (3.0.4)\n",
            "Installing collected packages: pyyaml, jarvis-md\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed jarvis-md-0.0.1a17 pyyaml-6.0\n"
          ]
        }
      ],
      "source": [
        "# --- Install jarvis (only in Google Colab or local runtime)\n",
        "% pip install jarvis-md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuFc0aaaPK9T"
      },
      "source": [
        "### Imports\n",
        "\n",
        "Use the following lines to import any additional needed libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M979B8ltPK9T"
      },
      "outputs": [],
      "source": [
        "import numpy as np, pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input, Model, models, layers, losses, metrics, optimizers\n",
        "from jarvis.train import datasets\n",
        "from jarvis.utils.display import imshow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMhl-K_LPK9U"
      },
      "source": [
        "# Data\n",
        "\n",
        "The data used in this tutorial will consist of kidney tumor CT exams derived from the Kidney Tumor Segmentation Challenge (KiTS). More information about the KiTS Challenge can be found here: https://kits21.kits-challenge.org/. The custom `datasets.download(...)` method can be used to download a local copy of the dataset. By default the dataset will be archived at `/data/raw/ct_kits`; as needed an alternate location may be specified using `datasets.download(name=..., path=...)`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Rpgoa7ssPK9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc62858f-eb3d-4d9c-f914-fe1f7351fbde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2022-06-08 18:04:49 ] [====================] 100.000% : Extracting archive (0000818 / 0000818) "
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'code': '/data/raw/ct_kits', 'data': '/data/raw/ct_kits'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# --- Download dataset\n",
        "datasets.download(name='ct/kits')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dJpEK7pPK9V"
      },
      "source": [
        "Since the algorithms below may require slightly different model inputs, the required generators and inputs will be defined dyanically in the code blocks later in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwMzJ66_PK9V"
      },
      "source": [
        "### Data Generator\n",
        "\n",
        "To accomodate these various permutations, consider the following custom code to implement a nested generator strategy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "J1H3-XJiPK9W"
      },
      "outputs": [],
      "source": [
        "def G(gen, dims=2, task='cls', binarize=True):\n",
        "    \"\"\"\n",
        "    Custom generator to modify raw labels for 2D/3D classification or segmentation tasks\n",
        "    \n",
        "    :params\n",
        "    \n",
        "      (generator) gen      : original unmodified generator\n",
        "      (int)       dims     : 2D or 3D model\n",
        "      (str)       task     : 'cls' or 'seg' \n",
        "      (bool)      binarize : whether or not to binarize original 3-class labels\n",
        "    \n",
        "    \"\"\"\n",
        "    assert task in ['cls', 'seg']\n",
        "\n",
        "    for xs, _ in gen:\n",
        "\n",
        "        # --- Convert segmentation into classification labels\n",
        "        if task == 'cls':\n",
        "            axis = (2, 3, 4) if dims == 2 else (1, 2, 3, 4)\n",
        "            xs['lbl'] = np.max(xs['lbl'], axis=axis, keepdims=True)\n",
        "            \n",
        "        # --- Binarize\n",
        "        if binarize:\n",
        "            xs['lbl'] = xs['lbl'] == 2\n",
        "\n",
        "        yield xs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eonofAiPK9W"
      },
      "source": [
        "# Training\n",
        "\n",
        "A total of three different network architectures will be tested. The goal is to compare the incremental benefit of several design choices. After building and training each model to convergence, do not forget to save each model as a separate `*.hdf5` file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfh5CyWqPK9X"
      },
      "source": [
        "## 1. Classification\n",
        "\n",
        "The first task is to create any classification model for binary tumor detection. A 2D model will predict tumor vs. no tumor on a slice-by-slice basis whereas a 3D model will predict tumor vs. no tumor on a volume basis. Regardless of implementation choice, all statistical analysis will be performed on a **volume basis**. For those that choose a 2D model, a reduction strategy must be implemented (see details further below)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cqf2QdNpPK9X"
      },
      "source": [
        "### Create generators\n",
        "\n",
        "Use the following code cells to choose either a 2D or 3D input. As needed, feel free to modify the batch size and/or implement stratified sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_qxQIzQPK9X"
      },
      "source": [
        "**2D dataset**: To select the 2D data of input size `(1, 96, 96, 1)` use the keyword `2d`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jzUb_2uBPK9X"
      },
      "outputs": [],
      "source": [
        "# --- Prepare generators\n",
        "configs = {'batch': {'size': 16, 'fold': 0}}\n",
        "gen_train, gen_valid, client = datasets.prepare(name='ct/kits', keyword='2d', configs=configs, custom_layers=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7EEteZCPK9Y"
      },
      "source": [
        "**3D dataset**: To select the 3D data of input size `(96, 96, 96, 1)` use the keyword `3d`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYa20UHiPK9Y"
      },
      "outputs": [],
      "source": [
        "# --- Prepare generators\n",
        "configs = {'batch': {'size': 2, 'fold': 0}}\n",
        "gen_train, gen_valid, client = datasets.prepare(name='ct/kits', keyword='3d', configs=configs, custom_layers=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb27PFUbPK9Y"
      },
      "source": [
        "### Define model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LNggus5CPK9Y"
      },
      "outputs": [],
      "source": [
        "kernel_size = (1, 3, 3) \n",
        "strides = (1, 2, 2)\n",
        "    \n",
        "    # --- Define kwargs\n",
        "kwargs = {\n",
        "        'kernel_size': kernel_size,\n",
        "        'padding': 'same',\n",
        "        'kernel_initializer': 'he_normal'}\n",
        "\n",
        "    # --- Define block componentsconv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
        "conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
        "tran = lambda x, filters, strides : layers.Conv3DTranspose(filters=filters, strides=strides, **kwargs)(x)\n",
        "\n",
        "norm = lambda x : layers.BatchNormalization()(x)\n",
        "relu = lambda x : layers.ReLU()(x)\n",
        "\n",
        "conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
        "conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=strides)))\n",
        "tran2 = lambda filters, x : relu(norm(tran(x, filters, strides=strides)))\n",
        "\n",
        "concat = lambda a, b : layers.Concatenate()([a, b])\n",
        "                                     \n",
        "x = Input(shape=(None, 96, 96, 1), dtype='float32')\n",
        "\n",
        "# --- Define projection\n",
        "bneck = lambda filters, x : layers.Conv3D(\n",
        "    filters=filters, \n",
        "    strides=1, \n",
        "    kernel_size=(1, 1, 1),\n",
        "    padding='same')(relu(norm(x)))\n",
        "l0 = conv(x, filters=16, strides=1)\n",
        "# --- Define standard conv-conv block\n",
        "l1 = conv1(32, l0)\n",
        "l2 = conv1(32, l1)\n",
        "\n",
        "# --- Define bottleneck conv-conv block\n",
        "l1 = conv1(32, l0)\n",
        "l2 = bneck(32, conv1(8, bneck(8, l1)))\n",
        "l3 = conv1(32, conv2(32, l2))\n",
        "l4 = conv1(48, conv2(48, l3))\n",
        "l5 = conv1(64, conv2(64, l4))\n",
        "l6 = conv1(80, conv2(80, l5))\n",
        "n0, n1, c = l6.shape[-3:]\n",
        "f0 = layers.Reshape([-1, 1, 1, n0 * n1 * c])(l6)\n",
        "\n",
        "# --- Create logits\n",
        "logits = layers.Conv3D(filters=2, kernel_size=1)(f0)\n",
        "\n",
        "# --- Create backbone model\n",
        "backbone = Model(inputs=x, outputs=logits)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sacNkTbUPK9Y"
      },
      "outputs": [],
      "source": [
        "inputs = {\n",
        "    'dat': Input(shape=(None, 96, 96, 1), name='dat'),\n",
        "    'lbl': Input(shape=(None, 1, 1, 1), name='lbl')}    \n",
        "logits = backbone(inputs['dat'])\n",
        "\n",
        "# --- Define loss object\n",
        "sce = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# --- Create loss tensor\n",
        "loss = sce(y_true=inputs['lbl'], y_pred=logits)\n",
        "# --- Define accuracy\n",
        "acc = metrics.sparse_categorical_accuracy(y_true=inputs['lbl'], y_pred=logits)\n",
        "# --- Create training model\n",
        "training = Model(inputs=inputs, outputs={'logits': logits, 'loss': loss, 'acc': acc})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzSmYQvQPK9Z"
      },
      "source": [
        "### Compile and train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "O-Jh1oO1PK9Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23a2b779-5a4d-4b05-a81d-3d47f662d75c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2022-06-08 18:05:32 ] [====================] 100.000% : Iterating | 000081    Acc: 0.6173\n",
            "Sen: 0.6170\n",
            "Spe: 0.6176\n",
            "PPV: 0.6905\n",
            "NPV: 0.5385\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 15s 118ms/step - loss: 0.7042 - acc: 0.6006\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 12s 118ms/step - loss: 0.5780 - acc: 0.7150\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 12s 118ms/step - loss: 0.5174 - acc: 0.7525\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 12s 124ms/step - loss: 0.4830 - acc: 0.7700\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 24s 239ms/step - loss: 0.4606 - acc: 0.7794 - val_loss: 0.8029 - val_acc: 0.6581\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 12s 120ms/step - loss: 0.4228 - acc: 0.8019\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 12s 119ms/step - loss: 0.4193 - acc: 0.8194\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 12s 124ms/step - loss: 0.3324 - acc: 0.8619\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 12s 120ms/step - loss: 0.3166 - acc: 0.8637\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 23s 227ms/step - loss: 0.3104 - acc: 0.8700 - val_loss: 0.8049 - val_acc: 0.6744\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f06009598d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# --- Create validation generator\n",
        "test_train, test_valid = client.create_generators(test=True, expand=True)\n",
        "test_train = G(test_train, dims=2, task='cls')\n",
        "test_valid = G(test_valid, dims=2, task='cls')\n",
        "\n",
        "preds = []\n",
        "trues = []\n",
        "\n",
        "for x in test_valid:\n",
        "    \n",
        "    # --- Aggregate preds\n",
        "    pred = backbone.predict(x['dat'])\n",
        "    preds.append(np.argmax(pred, axis=-1).sum())\n",
        "\n",
        "    # --- Aggregate trues\n",
        "    trues.append(x['lbl'].any())\n",
        "\n",
        "# --- Create Numpy arrays\n",
        "preds = np.array(preds)\n",
        "trues = np.array(trues)\n",
        "# --- Add loss\n",
        "training.add_loss(loss)\n",
        "\n",
        "# --- Add metric\n",
        "training.add_metric(acc, name='acc')\n",
        "\n",
        "# --- Define an Adam optimizer\n",
        "optimizer = optimizers.Adam(learning_rate=2e-4)\n",
        "\n",
        "# --- Compile model\n",
        "training.compile(optimizer=optimizer)\n",
        "\n",
        "# --- Apply threshold\n",
        "thresh = np.median(preds)\n",
        "preds_ = preds >= thresh\n",
        "# --- Calculate TP/TN/FN/FP\n",
        "corr = preds_ == trues\n",
        "tp = np.sum(corr & trues)\n",
        "tn = np.sum(corr & ~trues)\n",
        "fn = np.sum(~corr & trues)\n",
        "fp = np.sum(~corr & ~trues)\n",
        "\n",
        "# --- Calculate stats\n",
        "acc = (tp + tn) / corr.size\n",
        "sen = tp / (tp + fn)\n",
        "spe = tn / (tn + fp)\n",
        "ppv = tp / (tp + fp)\n",
        "npv = tn / (tn + fn)\n",
        "\n",
        "print('Acc: {:0.4f}'.format(acc))\n",
        "print('Sen: {:0.4f}'.format(sen))\n",
        "print('Spe: {:0.4f}'.format(spe))\n",
        "print('PPV: {:0.4f}'.format(ppv))\n",
        "print('NPV: {:0.4f}'.format(npv))\n",
        "\n",
        "# --- Train the model\n",
        "training.fit(\n",
        "    x=G(gen_train, dims=2, task='cls'),\n",
        "    validation_data=G(gen_valid, dims=2, task='cls'),\n",
        "    steps_per_epoch=100, \n",
        "    epochs=10,\n",
        "    validation_steps=100,\n",
        "    validation_freq=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Serialize a model\n",
        "backbone.save('./model1.hdf5')"
      ],
      "metadata": {
        "id": "K-QR1kYWHcpZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eedb225b-7bea-44da-b6d1-40f2130e99cb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Create validation generator\n",
        "test_train, test_valid = client.create_generators(test=True, expand=True)\n",
        "test_train = G(test_train, dims=2, task='cls')\n",
        "test_valid = G(test_valid, dims=2, task='cls')\n",
        "\n",
        "preds = []\n",
        "trues = []\n",
        "\n",
        "\n",
        "for x in test_train:\n",
        "    \n",
        "    # --- Aggregate preds\n",
        "    pred = backbone.predict(x['dat'])\n",
        "    preds.append(np.argmax(pred, axis=-1).sum())\n",
        "\n",
        "    # --- Aggregate trues\n",
        "    trues.append(x['lbl'].any())\n",
        "# --- Create Numpy arrays\n",
        "preds = np.array(preds)\n",
        "trues = np.array(trues)\n",
        "\n",
        "# --- Apply threshold\n",
        "thresh = np.median(preds)\n",
        "preds_ = preds >= thresh\n",
        "\n",
        "# --- Calculate TP/TN/FN/FP\n",
        "corr = preds_ == trues\n",
        "tp = np.sum(corr & trues)\n",
        "tn = np.sum(corr & ~trues)\n",
        "fn = np.sum(~corr & trues)\n",
        "fp = np.sum(~corr & ~trues)\n",
        "\n",
        "# --- Calculate stats\n",
        "acc = (tp + tn) / corr.size\n",
        "sen = tp / (tp + fn)\n",
        "spe = tn / (tn + fp)\n",
        "ppv = tp / (tp + fp)\n",
        "npv = tn / (tn + fn)\n",
        "\n",
        "print('Acc: {:0.4f}'.format(acc))\n",
        "print('Sen: {:0.4f}'.format(sen))\n",
        "print('Spe: {:0.4f}'.format(spe))\n",
        "print('PPV: {:0.4f}'.format(ppv))\n",
        "print('NPV: {:0.4f}'.format(npv))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzLxF0cpBfh-",
        "outputId": "1d85062a-e676-4d64-f743-b3513162fd5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2022-06-08 02:39:29 ] [====================] 100.000% : Iterating | 000321    Acc: 0.8536\n",
            "Sen: 0.8679\n",
            "Spe: 0.8395\n",
            "PPV: 0.8415\n",
            "NPV: 0.8662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKhAEdJ0PK9Z"
      },
      "source": [
        "## 2. Segmentation\n",
        "\n",
        "The second task is to create any segmentation model for binary tumor localization. A 2D model will predict tumor segmentation masks on a slice-by-slice basis whereas a 3D model will predict tumor segmentation masks on a volume basis. Regardless of implementation choice, all statistical analysis will be performed on a **volume basis**. To do so, a reduction strategy must be implemented (see details further below)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lf2Yiv70tzn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69sgStSDPK9Z"
      },
      "source": [
        "### Create generators\n",
        "\n",
        "Use the following code cells to choose either a 2D or 3D input. As needed, feel free to modify the batch size and/or implement stratified sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69bxQWGLPK9Z"
      },
      "source": [
        "**2D dataset**: To select the 2D data of input size `(1, 96, 96, 1)` use the keyword `2d`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziT-NbbaPK9a"
      },
      "outputs": [],
      "source": [
        "# --- Prepare generators\n",
        "configs = {'batch': {'size': 16, 'fold': 0}}\n",
        "gen_train, gen_valid, client = datasets.prepare(name='ct/kits', keyword='2d', configs=configs, custom_layers=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRBaRaH8PK9a"
      },
      "source": [
        "**3D dataset**: To select the 3D data of input size `(96, 96, 96, 1)` use the keyword `3d`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26NaXNVHPK9a"
      },
      "outputs": [],
      "source": [
        "# --- Prepare generators\n",
        "configs = {'batch': {'size': 2, 'fold': 0}}\n",
        "gen_train, gen_valid, client = datasets.prepare(name='ct/kits', keyword='3d', configs=configs, custom_layers=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP6BeyeBPK9a"
      },
      "source": [
        "### Define model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IlUQTbJPK9b"
      },
      "outputs": [],
      "source": [
        "# --- Define model input \n",
        "x = Input(shape=(None, 96, 96, 1), dtype='float32')\n",
        "# --- Define kwargs dictionary\n",
        "kernel_size = (1, 3, 3) \n",
        "strides = (1, 2, 2)\n",
        "    \n",
        "    # --- Define kwargs\n",
        "kwargs = {\n",
        "        'kernel_size': kernel_size,\n",
        "        'padding': 'same',\n",
        "        'kernel_initializer': 'he_normal'}\n",
        "\n",
        "    # --- Define block componentsconv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
        "\n",
        "\n",
        "# --- Define lambda functions\n",
        "conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
        "norm = lambda x : layers.BatchNormalization()(x)\n",
        "relu = lambda x : layers.ReLU()(x)\n",
        "\n",
        "# --- Define stride-1, stride-2 blocks\n",
        "conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
        "conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=(1, 2, 2))))# --- Define single transpose\n",
        "tran = lambda x, filters, strides : layers.Conv3DTranspose(filters=filters, strides=strides, **kwargs)(x)\n",
        "\n",
        "# --- Define transpose block\n",
        "tran2 = lambda filters, x : relu(norm(tran(x, filters, strides=(1, 2, 2))))\n",
        "# --- Define contracting layers\n",
        "l1 = conv1(8, x)\n",
        "l2 = conv1(16, conv2(16, l1))\n",
        "l3 = conv1(32, conv2(32, l2))\n",
        "l4 = conv1(48, conv2(48, l3))\n",
        "l5 = conv1(64, conv2(64, l4))\n",
        "l6  = tran2(48, l5)\n",
        "\n",
        "concat = lambda a, b : layers.Concatenate()([a, b])\n",
        "concat(l4, l6)\n",
        "\n",
        "# --- Define expanding layers\n",
        "l7  = tran2(32, conv1(48, concat(l4, l6)))\n",
        "l8  = tran2(16, conv1(32, concat(l3, l7)))\n",
        "l9  = tran2(8,  conv1(16, concat(l2, l8)))\n",
        "l10 = conv1(8,  l9)\n",
        "\n",
        "\n",
        "# --- Create logits\n",
        "logits = layers.Conv3D(filters=2, **kwargs)(l10)\n",
        "\n",
        "# --- Create model\n",
        "backbone = Model(inputs=x, outputs=logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuvEnNnYPK9b"
      },
      "outputs": [],
      "source": [
        "# --- Create training model\n",
        "inputs = {\n",
        "    'dat': Input(shape=(None, 96, 96, 1), name='dat'),\n",
        "    'lbl': Input(shape=(None, 96, 96, 1), name='lbl')}\n",
        "# --- Define first step of new wrapper model\n",
        "logits = backbone(inputs['dat'])\n",
        "# --- Define loss object\n",
        "sce = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# --- Create loss tensor\n",
        "loss = sce(y_true=inputs['lbl'], y_pred=logits)\n",
        "def create_weights(lbl, pos_weight=5.0):\n",
        "    \n",
        "    # --- Create wgt\n",
        "    wgt = tf.ones_like(lbl, dtype='float32')\n",
        "    wgt = wgt + tf.cast(lbl == 2, dtype='float32') * (pos_weight - 1.0)\n",
        "    \n",
        "    return wgt\n",
        "# --- Create weights\n",
        "wgt = create_weights(inputs['lbl'])\n",
        "def calculate_dsc(y_true, y_pred, weights=None, c=1):\n",
        "    \"\"\"\n",
        "    Method to calculate the Dice score coefficient for given class\n",
        "\n",
        "    :params\n",
        "\n",
        "      y_true : ground-truth label\n",
        "      y_pred : predicted logits scores\n",
        "           c : class to calculate DSC on\n",
        "\n",
        "    \"\"\"    \n",
        "    true = y_true[..., 0] == c\n",
        "    pred = tf.math.argmax(y_pred, axis=-1) == c \n",
        "\n",
        "    if weights is not None:\n",
        "        true = true & (weights[..., 0] != 0)\n",
        "        pred = pred & (weights[..., 0] != 0)\n",
        "\n",
        "    A = tf.math.count_nonzero(true & pred) * 2\n",
        "    B = tf.math.count_nonzero(true) + tf.math.count_nonzero(pred)\n",
        "\n",
        "    return tf.math.divide_no_nan(\n",
        "        tf.cast(A, tf.float32), \n",
        "        tf.cast(B, tf.float32))\n",
        "dsc = calculate_dsc(y_true=inputs['lbl'], y_pred=logits)\n",
        "training = Model(inputs=inputs, outputs={'logits': logits, 'loss': loss, 'dsc': dsc})\n",
        "# --- Add loss\n",
        "training.add_loss(loss)\n",
        "\n",
        "# --- Add metric\n",
        "training.add_metric(dsc, name='dsc')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIfCGDHXPK9b"
      },
      "source": [
        "### Compile and train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fp0p8uGPK9b"
      },
      "outputs": [],
      "source": [
        "# --- Define an Adam optimizer\n",
        "optimizer = optimizers.Adam(learning_rate=2e-4)\n",
        "\n",
        "# --- Compile model\n",
        "training.compile(optimizer=optimizer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Create validation generator\n",
        "test_train, test_valid = client.create_generators(test=True, expand=True)\n",
        "test_train = G(test_train, dims=2, task='seg')\n",
        "test_valid = G(test_valid, dims=2, task='seg')\n",
        "\n",
        "preds = []\n",
        "trues = []\n",
        "\n",
        "for x in test_valid:\n",
        "    \n",
        "    # --- Aggregate preds\n",
        "    pred = backbone.predict(x['dat'])\n",
        "    preds.append(np.argmax(pred, axis=-1).sum())\n",
        "\n",
        "    # --- Aggregate trues\n",
        "    trues.append(x['lbl'].any())\n",
        "\n",
        "# --- Create Numpy arrays\n",
        "preds = np.array(preds)\n",
        "trues = np.array(trues)\n",
        "# --- Apply threshold\n",
        "thresh = np.median(preds)\n",
        "preds_ = preds >= thresh\n",
        "# --- Calculate TP/TN/FN/FP\n",
        "corr = preds_ == trues\n",
        "tp = np.sum(corr & trues)\n",
        "tn = np.sum(corr & ~trues)\n",
        "fn = np.sum(~corr & trues)\n",
        "fp = np.sum(~corr & ~trues)\n",
        "\n",
        "# --- Calculate stats\n",
        "acc = (tp + tn) / corr.size\n",
        "sen = tp / (tp + fn)\n",
        "spe = tn / (tn + fp)\n",
        "ppv = tp / (tp + fp)\n",
        "npv = tn / (tn + fn)\n",
        "\n",
        "# --- Apply threshold\n",
        "thresh = np.median(preds)\n",
        "preds_ = preds >= thresh\n",
        "# --- Calculate TP/TN/FN/FP\n",
        "corr = preds_ == trues\n",
        "tp = np.sum(corr & trues)\n",
        "tn = np.sum(corr & ~trues)\n",
        "fn = np.sum(~corr & trues)\n",
        "fp = np.sum(~corr & ~trues)\n",
        "\n",
        "# --- Calculate stats\n",
        "acc = (tp + tn) / corr.size\n",
        "sen = tp / (tp + fn)\n",
        "spe = tn / (tn + fp)\n",
        "ppv = tp / (tp + fp)\n",
        "npv = tn / (tn + fn)\n",
        "\n",
        "\n",
        "\n",
        "# --- Train the model\n",
        "training.fit(\n",
        "    x=G(gen_train, dims=2, task='seg'),\n",
        "    validation_data=G(gen_valid, dims=2, task='seg'),\n",
        "    steps_per_epoch=100, \n",
        "    epochs=20,\n",
        "    validation_steps=100,\n",
        "    validation_freq=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7_o2Wu6pdXT",
        "outputId": "95e26415-feec-407e-8d6e-4ee29a5713f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2022-06-08 02:47:30 ] [====================] 100.000% : Iterating | 000081    Epoch 1/20\n",
            "100/100 [==============================] - 12s 120ms/step - loss: 0.0201 - dsc: 0.8040\n",
            "Epoch 2/20\n",
            "100/100 [==============================] - 12s 122ms/step - loss: 0.0214 - dsc: 0.8015\n",
            "Epoch 3/20\n",
            "100/100 [==============================] - 12s 124ms/step - loss: 0.0183 - dsc: 0.8288\n",
            "Epoch 4/20\n",
            "100/100 [==============================] - 12s 124ms/step - loss: 0.0179 - dsc: 0.8450\n",
            "Epoch 5/20\n",
            "100/100 [==============================] - 24s 242ms/step - loss: 0.0173 - dsc: 0.8460 - val_loss: 0.0810 - val_dsc: 0.4128\n",
            "Epoch 6/20\n",
            "100/100 [==============================] - 13s 126ms/step - loss: 0.0163 - dsc: 0.8444\n",
            "Epoch 7/20\n",
            "100/100 [==============================] - 13s 130ms/step - loss: 0.0148 - dsc: 0.8555\n",
            "Epoch 8/20\n",
            "100/100 [==============================] - 12s 125ms/step - loss: 0.0150 - dsc: 0.8651\n",
            "Epoch 9/20\n",
            "100/100 [==============================] - 12s 124ms/step - loss: 0.0139 - dsc: 0.8738\n",
            "Epoch 10/20\n",
            "100/100 [==============================] - 24s 236ms/step - loss: 0.0131 - dsc: 0.8816 - val_loss: 0.0803 - val_dsc: 0.4761\n",
            "Epoch 11/20\n",
            "100/100 [==============================] - 12s 121ms/step - loss: 0.0135 - dsc: 0.8776\n",
            "Epoch 12/20\n",
            "100/100 [==============================] - 12s 121ms/step - loss: 0.0126 - dsc: 0.8773\n",
            "Epoch 13/20\n",
            "100/100 [==============================] - 12s 121ms/step - loss: 0.0116 - dsc: 0.8865\n",
            "Epoch 14/20\n",
            "100/100 [==============================] - 12s 123ms/step - loss: 0.0121 - dsc: 0.8915\n",
            "Epoch 15/20\n",
            "100/100 [==============================] - 23s 234ms/step - loss: 0.0111 - dsc: 0.8898 - val_loss: 0.0790 - val_dsc: 0.4751\n",
            "Epoch 16/20\n",
            "100/100 [==============================] - 12s 122ms/step - loss: 0.0107 - dsc: 0.8987\n",
            "Epoch 17/20\n",
            "100/100 [==============================] - 12s 122ms/step - loss: 0.0110 - dsc: 0.8951\n",
            "Epoch 18/20\n",
            "100/100 [==============================] - 12s 122ms/step - loss: 0.0100 - dsc: 0.9077\n",
            "Epoch 19/20\n",
            "100/100 [==============================] - 12s 121ms/step - loss: 0.0112 - dsc: 0.8930\n",
            "Epoch 20/20\n",
            "100/100 [==============================] - 23s 236ms/step - loss: 0.0103 - dsc: 0.9058 - val_loss: 0.0706 - val_dsc: 0.5633\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f89d9039450>"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Serialize a model\n",
        "backbone.save('./model2.hdf5')"
      ],
      "metadata": {
        "id": "_Vglvu_hHfri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00e999cb-df68-481c-f903-46f7d162ac92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Acc: {:0.4f}'.format(acc))\n",
        "print('Sen: {:0.4f}'.format(sen))\n",
        "print('Spe: {:0.4f}'.format(spe))\n",
        "print('PPV: {:0.4f}'.format(ppv))\n",
        "print('NPV: {:0.4f}'.format(npv))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gQMC63CDJqG",
        "outputId": "588bf862-65c2-486b-af7d-80ba3a299e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc: 0.5802\n",
            "Sen: 0.5745\n",
            "Spe: 0.5882\n",
            "PPV: 0.6585\n",
            "NPV: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Create validation generator\n",
        "test_train, test_valid = client.create_generators(test=True, expand=True)\n",
        "test_train = G(test_train, dims=2, task='seg')\n",
        "test_valid = G(test_valid, dims=2, task='seg')\n",
        "\n",
        "preds = []\n",
        "trues = []\n",
        "\n",
        "\n",
        "for x in test_train:\n",
        "    \n",
        "    # --- Aggregate preds\n",
        "    pred = backbone.predict(x['dat'])\n",
        "    preds.append(np.argmax(pred, axis=-1).sum())\n",
        "\n",
        "    # --- Aggregate trues\n",
        "    trues.append(x['lbl'].any())\n",
        "# --- Create Numpy arrays\n",
        "preds = np.array(preds)\n",
        "trues = np.array(trues)\n",
        "\n",
        "# --- Apply threshold\n",
        "thresh = np.median(preds)\n",
        "preds_ = preds >= thresh\n",
        "\n",
        "# --- Calculate TP/TN/FN/FP\n",
        "corr = preds_ == trues\n",
        "tp = np.sum(corr & trues)\n",
        "tn = np.sum(corr & ~trues)\n",
        "fn = np.sum(~corr & trues)\n",
        "fp = np.sum(~corr & ~trues)\n",
        "\n",
        "# --- Calculate stats\n",
        "acc = (tp + tn) / corr.size\n",
        "sen = tp / (tp + fn)\n",
        "spe = tn / (tn + fp)\n",
        "ppv = tp / (tp + fp)\n",
        "npv = tn / (tn + fn)\n",
        "\n",
        "print('Acc: {:0.4f}'.format(acc))\n",
        "print('Sen: {:0.4f}'.format(sen))\n",
        "print('Spe: {:0.4f}'.format(spe))\n",
        "print('PPV: {:0.4f}'.format(ppv))\n",
        "print('NPV: {:0.4f}'.format(npv))"
      ],
      "metadata": {
        "id": "T3Qnr_4WHOCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db651d9f-782d-4a09-daf9-87dbec2f76cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2022-06-08 02:56:01 ] [====================] 100.000% : Iterating | 000321    Acc: 0.9626\n",
            "Sen: 0.9686\n",
            "Spe: 0.9568\n",
            "PPV: 0.9565\n",
            "NPV: 0.9688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx170xMhPK9b"
      },
      "source": [
        "## 3. Custom architecture\n",
        "\n",
        "Finally, using any of the customizations described in class, find a top-performing model that may potentially yield some incremental benefit over the two baseline models above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sivycOY6PK9b"
      },
      "source": [
        "### Create generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrmAPY3VPK9b"
      },
      "outputs": [],
      "source": [
        "# --- Prepare generators\n",
        "configs = {'batch': {'size': 16, 'fold': 0}}\n",
        "gen_train, gen_valid, client = datasets.prepare(name='ct/kits', keyword='2d', configs=configs, custom_layers=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gUcePEUPK9c"
      },
      "source": [
        "### Define model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-Fte6mnPK9c"
      },
      "outputs": [],
      "source": [
        "# --- Define model input \n",
        "x = Input(shape=(None, 96, 96, 1), dtype='float32')\n",
        "# --- Define kwargs dictionary\n",
        "kernel_size = (1, 3, 3) \n",
        "strides = (1, 2, 2)\n",
        "    \n",
        "    # --- Define kwargs\n",
        "kwargs = {\n",
        "        'kernel_size': kernel_size,\n",
        "        'padding': 'same',\n",
        "        'kernel_initializer': 'he_normal'}\n",
        "# --- Define lambda functions\n",
        "conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
        "norm = lambda x : layers.BatchNormalization()(x)\n",
        "relu = lambda x : layers.ReLU()(x)\n",
        "\n",
        "# --- Define stride-1, stride-2 blocks\n",
        "conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
        "conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=(1, 2, 2))))\n",
        "\n",
        "# --- Define model\n",
        "l1 = conv1(32,  x)\n",
        "# --- Squeeze (global pool)\n",
        "sqz = layers.AveragePooling3D((1, l1.shape[2], l1.shape[3]))(l1)\n",
        "\n",
        "# --- Excitation (reduce channels to 1 / R) ==> in this example set R = 4 arbitrarily\n",
        "cha = int(l1.shape[-1] / 4)\n",
        "exc = layers.Conv3D(filters=cha, kernel_size=1, activation='relu')(sqz)\n",
        "\n",
        "# --- Scale (expand channels to original size)\n",
        "sca = layers.Conv3D(filters=l1.shape[-1], kernel_size=1, activation='sigmoid')(exc)\n",
        "\n",
        "# --- Modify l1\n",
        "l1 = l1 * sca\n",
        "# --- Define model\n",
        "l1 = conv1(32,  x)\n",
        "l2 = conv1(48,  conv2(48,  l1))\n",
        "l3 = conv1(64,  conv2(64,  l2))\n",
        "l4 = conv1(80,  conv2(80,  l3))\n",
        "l5 = conv1(96,  conv2(96,  l4))\n",
        "l6 = conv1(128, conv2(128, l5))\n",
        "# --- Extract shape and reshape\n",
        "n0, n1, c = l6.shape[-3:]\n",
        "f0 = layers.Reshape([-1, 1, 1, n0 * n1 * c])(l6)\n",
        "\n",
        "# --- Create logits\n",
        "logits = layers.Conv3D(filters=2, kernel_size=1)(f0)\n",
        "\n",
        "# --- Create model\n",
        "backbone = Model(inputs=x, outputs=logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTe80fYvPK9c"
      },
      "outputs": [],
      "source": [
        "inputs = {\n",
        "    'dat': Input(shape=(None, 96, 96, 1), name='dat'),\n",
        "    'lbl': Input(shape=(None, 96, 96, 1), name='lbl')}\n",
        "# --- Define first step of new wrapper model\n",
        "logits = backbone(inputs['dat'])\n",
        "# --- Define loss object\n",
        "sce = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# --- Create loss tensor\n",
        "loss = sce(y_true=inputs['lbl'], y_pred=logits)\n",
        "# --- Create weights\n",
        "def create_weights(lbl, pos_weight=5.0):\n",
        "\n",
        "    wgt = tf.cast(lbl > 0, dtype='float32')\n",
        "    wgt = wgt + tf.cast(lbl == 2, dtype='float') * (pos_weight - 1.0)\n",
        "    return wgt\n",
        "    \n",
        "wgt = create_weights(inputs['lbl'])\n",
        "\n",
        "# --- Create y_true (binarized ground-truth)\n",
        "y_true = tf.cast(inputs['lbl'] == 2, dtype='uint8')\n",
        "\n",
        "# --- Create loss\n",
        "sce = losses.SparseCategoricalCrossentropy(from_logits=True)(\n",
        "    y_true=y_true,\n",
        "    y_pred=logits,\n",
        "    sample_weight=wgt)\n",
        "\n",
        "def calculate_dsc(y_true, y_pred, weights=None, c=1):\n",
        "    \"\"\"\n",
        "    Method to calculate the Dice score coefficient for given class\n",
        "\n",
        "    :params\n",
        "\n",
        "      y_true : ground-truth label\n",
        "      y_pred : predicted logits scores\n",
        "           c : class to calculate DSC on\n",
        "\n",
        "    \"\"\"    \n",
        "    true = y_true[..., 0] == c\n",
        "    pred = tf.math.argmax(y_pred, axis=-1) == c \n",
        "\n",
        "    if weights is not None:\n",
        "        true = true & (weights[..., 0] != 0)\n",
        "        pred = pred & (weights[..., 0] != 0)\n",
        "\n",
        "    A = tf.math.count_nonzero(true & pred) * 2\n",
        "    B = tf.math.count_nonzero(true) + tf.math.count_nonzero(pred)\n",
        "\n",
        "    return tf.math.divide_no_nan(\n",
        "        tf.cast(A, tf.float32), \n",
        "        tf.cast(B, tf.float32))\n",
        "dsc = calculate_dsc(y_true=inputs['lbl'], y_pred=logits)\n",
        "\n",
        "training = Model(inputs=inputs, outputs={'logits': logits, 'loss': loss, 'dsc': dsc})\n",
        "# --- Add loss\n",
        "training.add_loss(loss)\n",
        "\n",
        "# --- Add metric\n",
        "training.add_metric(dsc, name='dsc')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G712ifeZPK9d"
      },
      "source": [
        "### Compile and train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaoodwYdPK9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd85868e-f873-4fb1-866d-765eb20ca86c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2022-06-08 03:22:11 ] [====================] 100.000% : Iterating | 000081    Acc: 0.6543\n",
            "Sen: 0.6383\n",
            "Spe: 0.6765\n",
            "PPV: 0.7317\n",
            "NPV: 0.5750\n"
          ]
        }
      ],
      "source": [
        "# --- Define an Adam optimizer\n",
        "optimizer = optimizers.Adam(learning_rate=2e-4)\n",
        "\n",
        "# --- Compile model\n",
        "training.compile(optimizer=optimizer)\n",
        "# --- Load data into memory for faster training\n",
        "# --- Create validation generator\n",
        "test_train, test_valid = client.create_generators(test=True, expand=True)\n",
        "test_train = G(test_train, dims=2, task='cls')\n",
        "test_valid = G(test_valid, dims=2, task='cls')\n",
        "\n",
        "preds = []\n",
        "trues = []\n",
        "\n",
        "for x in test_valid:\n",
        "    \n",
        "    # --- Aggregate preds\n",
        "    pred = backbone.predict(x['dat'])\n",
        "    preds.append(np.argmax(pred, axis=-1).sum())\n",
        "\n",
        "    # --- Aggregate trues\n",
        "    trues.append(x['lbl'].any())\n",
        "\n",
        "# --- Create Numpy arrays\n",
        "preds = np.array(preds)\n",
        "trues = np.array(trues)\n",
        "# --- Apply threshold\n",
        "thresh = np.median(preds)\n",
        "preds_ = preds >= thresh\n",
        "# --- Apply threshold\n",
        "thresh = np.median(preds)\n",
        "preds_ = preds >= thresh\n",
        "\n",
        "# --- Calculate TP/TN/FN/FP\n",
        "corr = preds_ == trues\n",
        "tp = np.sum(corr & trues)\n",
        "tn = np.sum(corr & ~trues)\n",
        "fn = np.sum(~corr & trues)\n",
        "fp = np.sum(~corr & ~trues)\n",
        "\n",
        "# --- Calculate stats\n",
        "acc = (tp + tn) / corr.size\n",
        "sen = tp / (tp + fn)\n",
        "spe = tn / (tn + fp)\n",
        "ppv = tp / (tp + fp)\n",
        "npv = tn / (tn + fn)\n",
        "\n",
        "print('Acc: {:0.4f}'.format(acc))\n",
        "print('Sen: {:0.4f}'.format(sen))\n",
        "print('Spe: {:0.4f}'.format(spe))\n",
        "print('PPV: {:0.4f}'.format(ppv))\n",
        "print('NPV: {:0.4f}'.format(npv))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Train the model\n",
        "training.fit(\n",
        "    x=G(gen_train, dims=2, task='cls'),\n",
        "    validation_data=G(gen_valid, dims=2, task='cls'),\n",
        "    steps_per_epoch=100, \n",
        "    epochs=50,\n",
        "    validation_steps=100,\n",
        "    validation_freq=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZL8EQMdgOlc",
        "outputId": "85d2c6c0-7ce3-4e52-f90c-d262dc6ce27d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "100/100 [==============================] - 12s 122ms/step - loss: 0.0471 - dsc: 0.9758\n",
            "Epoch 2/50\n",
            "100/100 [==============================] - 12s 121ms/step - loss: 0.0323 - dsc: 0.9811\n",
            "Epoch 3/50\n",
            "100/100 [==============================] - 12s 121ms/step - loss: 0.0481 - dsc: 0.9789\n",
            "Epoch 4/50\n",
            "100/100 [==============================] - 12s 121ms/step - loss: 0.0484 - dsc: 0.9822\n",
            "Epoch 5/50\n",
            "100/100 [==============================] - 23s 235ms/step - loss: 0.0275 - dsc: 0.9883 - val_loss: 1.5481 - val_dsc: 0.5516\n",
            "Epoch 6/50\n",
            "100/100 [==============================] - 12s 120ms/step - loss: 0.0299 - dsc: 0.9910\n",
            "Epoch 7/50\n",
            "100/100 [==============================] - 12s 122ms/step - loss: 0.0768 - dsc: 0.9700\n",
            "Epoch 8/50\n",
            "100/100 [==============================] - 12s 122ms/step - loss: 0.0680 - dsc: 0.9753\n",
            "Epoch 9/50\n",
            "100/100 [==============================] - 12s 123ms/step - loss: 0.0447 - dsc: 0.9828\n",
            "Epoch 10/50\n",
            "100/100 [==============================] - 23s 232ms/step - loss: 0.0799 - dsc: 0.9596 - val_loss: 1.3441 - val_dsc: 0.5510\n",
            "Epoch 11/50\n",
            "100/100 [==============================] - 12s 122ms/step - loss: 0.0460 - dsc: 0.9782\n",
            "Epoch 12/50\n",
            "100/100 [==============================] - 12s 123ms/step - loss: 0.0355 - dsc: 0.9897\n",
            "Epoch 13/50\n",
            "100/100 [==============================] - 12s 124ms/step - loss: 0.0554 - dsc: 0.9709\n",
            "Epoch 14/50\n",
            "100/100 [==============================] - 12s 122ms/step - loss: 0.0407 - dsc: 0.9849\n",
            "Epoch 15/50\n",
            "100/100 [==============================] - 24s 237ms/step - loss: 0.0315 - dsc: 0.9878 - val_loss: 1.6784 - val_dsc: 0.5103\n",
            "Epoch 16/50\n",
            "100/100 [==============================] - 13s 128ms/step - loss: 0.0291 - dsc: 0.9890\n",
            "Epoch 17/50\n",
            "100/100 [==============================] - 12s 121ms/step - loss: 0.0350 - dsc: 0.9794\n",
            "Epoch 18/50\n",
            "100/100 [==============================] - 12s 121ms/step - loss: 0.0392 - dsc: 0.9820\n",
            "Epoch 19/50\n",
            "100/100 [==============================] - 12s 122ms/step - loss: 0.0498 - dsc: 0.9773\n",
            "Epoch 20/50\n",
            "100/100 [==============================] - 23s 233ms/step - loss: 0.0797 - dsc: 0.9766 - val_loss: 1.4041 - val_dsc: 0.5243\n",
            "Epoch 21/50\n",
            "100/100 [==============================] - 12s 124ms/step - loss: 0.0355 - dsc: 0.9845\n",
            "Epoch 22/50\n",
            "100/100 [==============================] - 12s 122ms/step - loss: 0.0380 - dsc: 0.9852\n",
            "Epoch 23/50\n",
            "100/100 [==============================] - 12s 122ms/step - loss: 0.0312 - dsc: 0.9841\n",
            "Epoch 24/50\n",
            "100/100 [==============================] - 12s 120ms/step - loss: 0.0278 - dsc: 0.9867\n",
            "Epoch 25/50\n",
            "100/100 [==============================] - 24s 237ms/step - loss: 0.0486 - dsc: 0.9801 - val_loss: 2.3083 - val_dsc: 0.3264\n",
            "Epoch 26/50\n",
            "100/100 [==============================] - 12s 122ms/step - loss: 0.0337 - dsc: 0.9848\n",
            "Epoch 27/50\n",
            "100/100 [==============================] - 12s 120ms/step - loss: 0.0556 - dsc: 0.9700\n",
            "Epoch 28/50\n",
            "100/100 [==============================] - 12s 123ms/step - loss: 0.0373 - dsc: 0.9849\n",
            "Epoch 29/50\n",
            "100/100 [==============================] - 12s 121ms/step - loss: 0.0430 - dsc: 0.9845\n",
            "Epoch 30/50\n",
            "100/100 [==============================] - 23s 233ms/step - loss: 0.0392 - dsc: 0.9810 - val_loss: 1.7172 - val_dsc: 0.4672\n",
            "Epoch 31/50\n",
            "100/100 [==============================] - 12s 121ms/step - loss: 0.0388 - dsc: 0.9826\n",
            "Epoch 32/50\n",
            "100/100 [==============================] - 12s 121ms/step - loss: 0.0413 - dsc: 0.9880\n",
            "Epoch 33/50\n",
            "100/100 [==============================] - 12s 122ms/step - loss: 0.0412 - dsc: 0.9840\n",
            "Epoch 34/50\n",
            "100/100 [==============================] - 12s 121ms/step - loss: 0.0511 - dsc: 0.9844\n",
            "Epoch 35/50\n",
            "100/100 [==============================] - 24s 238ms/step - loss: 0.0361 - dsc: 0.9841 - val_loss: 1.7844 - val_dsc: 0.4078\n",
            "Epoch 36/50\n",
            "100/100 [==============================] - 13s 127ms/step - loss: 0.0288 - dsc: 0.9923\n",
            "Epoch 37/50\n",
            "100/100 [==============================] - 12s 122ms/step - loss: 0.0339 - dsc: 0.9855\n",
            "Epoch 38/50\n",
            "100/100 [==============================] - 12s 121ms/step - loss: 0.0364 - dsc: 0.9839\n",
            "Epoch 39/50\n",
            "100/100 [==============================] - 12s 121ms/step - loss: 0.0244 - dsc: 0.9923\n",
            "Epoch 40/50\n",
            "100/100 [==============================] - 24s 236ms/step - loss: 0.0170 - dsc: 0.9910 - val_loss: 1.3945 - val_dsc: 0.5517\n",
            "Epoch 41/50\n",
            "100/100 [==============================] - 12s 122ms/step - loss: 0.0350 - dsc: 0.9917\n",
            "Epoch 42/50\n",
            "100/100 [==============================] - 12s 124ms/step - loss: 0.0365 - dsc: 0.9815\n",
            "Epoch 43/50\n",
            "100/100 [==============================] - 12s 121ms/step - loss: 0.0528 - dsc: 0.9832\n",
            "Epoch 44/50\n",
            "100/100 [==============================] - 12s 123ms/step - loss: 0.0423 - dsc: 0.9800\n",
            "Epoch 45/50\n",
            "100/100 [==============================] - 23s 235ms/step - loss: 0.0442 - dsc: 0.9893 - val_loss: 1.4945 - val_dsc: 0.5373\n",
            "Epoch 46/50\n",
            "100/100 [==============================] - 12s 122ms/step - loss: 0.0295 - dsc: 0.9906\n",
            "Epoch 47/50\n",
            "100/100 [==============================] - 12s 120ms/step - loss: 0.0436 - dsc: 0.9841\n",
            "Epoch 48/50\n",
            "100/100 [==============================] - 12s 122ms/step - loss: 0.0272 - dsc: 0.9904\n",
            "Epoch 49/50\n",
            "100/100 [==============================] - 12s 121ms/step - loss: 0.0240 - dsc: 0.9937\n",
            "Epoch 50/50\n",
            "100/100 [==============================] - 24s 237ms/step - loss: 0.0276 - dsc: 0.9861 - val_loss: 1.5390 - val_dsc: 0.5401\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8949542c50>"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Serialize a model\n",
        "backbone.save('./model3.hdf5')"
      ],
      "metadata": {
        "id": "DrxtsBV8HiXh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6968590a-310b-420e-b895-2594385e4808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Create validation generator\n",
        "test_train, test_valid = client.create_generators(test=True, expand=True)\n",
        "test_train = G(test_train, dims=2, task='cls')\n",
        "test_valid = G(test_valid, dims=2, task='cls')\n",
        "\n",
        "preds = []\n",
        "trues = []\n",
        "\n",
        "\n",
        "for x in test_train:\n",
        "    \n",
        "    # --- Aggregate preds\n",
        "    pred = backbone.predict(x['dat'])\n",
        "    preds.append(np.argmax(pred, axis=-1).sum())\n",
        "\n",
        "    # --- Aggregate trues\n",
        "    trues.append(x['lbl'].any())\n",
        "# --- Create Numpy arrays\n",
        "preds = np.array(preds)\n",
        "trues = np.array(trues)\n",
        "\n",
        "# --- Apply threshold\n",
        "thresh = np.median(preds)\n",
        "preds_ = preds >= thresh\n",
        "\n",
        "# --- Calculate TP/TN/FN/FP\n",
        "corr = preds_ == trues\n",
        "tp = np.sum(corr & trues)\n",
        "tn = np.sum(corr & ~trues)\n",
        "fn = np.sum(~corr & trues)\n",
        "fp = np.sum(~corr & ~trues)\n",
        "\n",
        "# --- Calculate stats\n",
        "acc = (tp + tn) / corr.size\n",
        "sen = tp / (tp + fn)\n",
        "spe = tn / (tn + fp)\n",
        "ppv = tp / (tp + fp)\n",
        "npv = tn / (tn + fn)\n",
        "\n",
        "print('Acc: {:0.4f}'.format(acc))\n",
        "print('Sen: {:0.4f}'.format(sen))\n",
        "print('Spe: {:0.4f}'.format(spe))\n",
        "print('PPV: {:0.4f}'.format(ppv))\n",
        "print('NPV: {:0.4f}'.format(npv))"
      ],
      "metadata": {
        "id": "EbMWjXr1HRMZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7218b706-330c-413b-df7a-365be31650e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2022-06-08 03:23:32 ] [====================] 100.000% : Iterating | 000321    Acc: 0.9938\n",
            "Sen: 1.0000\n",
            "Spe: 0.9877\n",
            "PPV: 0.9876\n",
            "NPV: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R688QkrQPK9d"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "For each of the three models, the following metrics should be calculated for **both the training and validation** cohorts:\n",
        "\n",
        "* accuracy\n",
        "* sensitivity\n",
        "* specificity\n",
        "* positive predictive value (PPV)\n",
        "* negative predictive value (NPV)\n",
        "\n",
        "As in prior assignments, accuracy is determined on a patient by patient (volume by volume) basis, so please implement a prediction reduction strategy as needed for your models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vg0HQiU3PK9d"
      },
      "outputs": [],
      "source": [
        "# --- Create validation generator\n",
        "test_train, test_valid = client.create_generators(test=True, expand=True)\n",
        "test_train = G(test_train, dims=2, task='cls')\n",
        "test_valid = G(test_valid, dims=2, task='cls')\n",
        "\n",
        "preds = []\n",
        "trues = []\n",
        "\n",
        "for x in test_valid:\n",
        "    \n",
        "    # --- Aggregate preds\n",
        "    pred = backbone.predict(x['dat'])\n",
        "    preds.append(np.argmax(pred, axis=-1).sum())\n",
        "\n",
        "    # --- Aggregate trues\n",
        "    trues.append(x['lbl'].any())\n",
        "\n",
        "# --- Create Numpy arrays\n",
        "preds = np.array(preds)\n",
        "trues = np.array(trues)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvKwInh9PK9d"
      },
      "source": [
        "### Performance\n",
        "\n",
        "The following minimum **validation cohort** performance metrics must be met for full credit:\n",
        "\n",
        "1. **Classification**: accuracy > 0.55\n",
        "2. **Segmentation**: accuracy > 0.55\n",
        "3. **Custom architecture**: accuracy > 0.60\n",
        "\n",
        "**Bonus**: the top three overall models based on **validation cohort** accuracy will recieve a +5 point (+15%) extra credit towards the final assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwP6EPAvPK9d"
      },
      "source": [
        "### Results\n",
        "\n",
        "When ready, create a `*.csv` file with your compiled **training and validation** cohort statistics for the three different models. Consider the following table format (although any format that contains the required information is sufficient):\n",
        "\n",
        "```\n",
        "          TRAINING                              VALIDATION\n",
        "          accuracy | sens | spec | PPV |  NPV | accuracy | sens | spec | PPV |  NPV\n",
        "model 1\n",
        "model 2\n",
        "model 3\n",
        "```\n",
        "\n",
        "As above, statistics for both training and validation should be provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5UD23U9PK9d"
      },
      "outputs": [],
      "source": [
        "# --- Create *.csv\n",
        "                              \n",
        "# --- Serialize *.csv\n",
        "df.to_csv('./results.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90NAXKoFPK9e"
      },
      "source": [
        "# Summary\n",
        "\n",
        "In addition to algorithm training as above, a 1-2 page write-up is required for this project. The goal is to *briefly* summarize algorithm design and key results. The write-up should be divided into three sections: methods; results; discussion. More detailed information and tips can be found here: https://github.com/peterchang77/dl_tutor/blob/master/cs190/spring_2021/notebooks/midterm/checklist.md.\n",
        "\n",
        "### Methods\n",
        "\n",
        "In this section, include details such as:\n",
        "\n",
        "* **Data**: How much data was used. How many cases were utilized for training and validation?\n",
        "* **Network design**: What are the different network architectures? How many layers and parameters? Were 2D or 3D operations used? Recall that the `model.summary(...)` can be used to provide key summary statistics for this purpose. If desired, feel free to include a model figure or diagram.\n",
        "* **Implementation**: How was training implemented. What are the key hyperparameters (e.g. learning rate, batch size, optimizer, etc)? How many training iterations were required for convergence? Did these hyperparameters change during the course of training?\n",
        "* **Statistics**: What statistics do you plan to use to evaluate model accuracy? \n",
        "\n",
        "### Results\n",
        "\n",
        "In this section, briefly summarize experimental results (a few sentences), and include the result table(s) as derived above.\n",
        "\n",
        "### Discussion\n",
        "\n",
        "Were the results expected or unexpected? What accounts for the differences in performance between the algorithms? How did you choose the network architecture implemented in your final model? Feel free to elaborate on any additional observations noted during the course of this expierment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCpIxXeFPK9e"
      },
      "source": [
        "# Submission\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPbU_g6QPK9e"
      },
      "source": [
        "### Canvas\n",
        "\n",
        "Once you have completed the midterm assignment, download the necessary files from Google Colab and your Google Drive. As in prior assigments, be sure to prepare:\n",
        "\n",
        "* final (completed) notebook: `[UCInetID]_assignment.ipynb`\n",
        "* final (results) spreadsheet: `[UCInetID]_results.csv` (compiled for all three parts)\n",
        "* final (trained) model: `[UCInetID]_model.hdf5` (three separate files for all three parts)\n",
        "\n",
        "In addition, submit the summary write-up as in any common document format (`.docx`, `.tex`, `.pdf`, etc):\n",
        "\n",
        "* final summary write-up: `[UCInetID]_summary.[docx|tex|pdf]`\n",
        "\n",
        "**Important**: please submit all your files prefixed with your UCInetID as listed above. Your UCInetID is the part of your UCI email address that comes before `@uci.edu`. For example, Peter Anteater has an email address of panteater@uci.edu, so his notebooke file would be submitted under the name `panteater_notebook.ipynb`, his spreadsheet would be submitted under the name `panteater_results.csv` and and his model file would be submitted under the name `panteater_model.hdf5`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Copy_of_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}